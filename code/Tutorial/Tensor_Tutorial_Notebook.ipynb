{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From:\n",
    "https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html\n",
    "\n",
    "\n",
    "Environment was created with:\n",
    "conda create --name  Nystromformer python=3.7\n",
    "conda install -n Nystromformer pip\n",
    "conda activate Nystromformer\n",
    "\n",
    "!pip install torch==1.7.1\n",
    "!pip install torchvision==0.8.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.4379, 0.3335],\n",
      "        [0.1629, 0.5807]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from another tensor\n",
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.7492, 0.0650, 0.3049],\n",
      "        [0.9043, 0.7244, 0.4119]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#shape\n",
    "shape = (2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Tensor attributes\n",
    "\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3791, 0.6023, 0.4230, 0.4251],\n",
       "        [0.3014, 0.8433, 0.7345, 0.7682],\n",
       "        [0.4361, 0.9318, 0.9461, 0.8927]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[0.1437, 0.3628, 0.1789, 0.1807],\n",
      "        [0.0908, 0.7111, 0.5394, 0.5901],\n",
      "        [0.1902, 0.8682, 0.8951, 0.7969]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[0.1437, 0.3628, 0.1789, 0.1807],\n",
      "        [0.0908, 0.7111, 0.5394, 0.5901],\n",
      "        [0.1902, 0.8682, 0.8951, 0.7969]])\n"
     ]
    }
   ],
   "source": [
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3791, 0.6023, 0.4230, 0.4251],\n",
      "        [0.3014, 0.8433, 0.7345, 0.7682],\n",
      "        [0.4361, 0.9318, 0.9461, 0.8927]]) \n",
      "\n",
      "tensor([[5.3791, 5.6023, 5.4230, 5.4251],\n",
      "        [5.3014, 5.8433, 5.7345, 5.7682],\n",
      "        [5.4361, 5.9318, 5.9461, 5.8927]])\n"
     ]
    }
   ],
   "source": [
    "# inplace operators have a suffix _\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda remove pytorch torchvision -y\n",
    "# !pip uninstall torch -y\n",
    "# !pip uninstall torch -y  # yes twice\n",
    "# !pip3 uninstall -y torch torchvision\n",
    "# !pip3 uninstall -y torch torchvision\n",
    "# !pip uninstall -y torch torchvision\n",
    "# !pip uninstall -y torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision==0.8.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Letâ€™s take a look at a single training step. For this example, we load a pretrained resnet18 model from torchvision. We create a random data tensor to represent a single image with 3 channels, and height & width of 64, and its corresponding label initialized to some random values.\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.0098e-02, 4.4756e-01, 4.5881e-01, 5.4276e-02, 5.4434e-01, 5.2426e-01,\n",
       "         4.4787e-02, 1.6680e-02, 1.9464e-01, 4.4486e-01, 8.2914e-01, 6.7444e-01,\n",
       "         8.4176e-01, 1.3151e-01, 6.5901e-02, 1.6886e-01, 5.3271e-01, 3.2581e-01,\n",
       "         2.4316e-01, 5.7487e-01, 1.0449e-01, 5.5043e-01, 2.8974e-01, 1.5953e-01,\n",
       "         7.8492e-01, 8.3033e-01, 6.8157e-01, 9.0992e-01, 4.6673e-01, 7.8064e-01,\n",
       "         1.4195e-02, 2.4048e-01, 8.3334e-01, 5.9467e-01, 2.2144e-01, 8.1356e-01,\n",
       "         7.4459e-01, 3.6248e-01, 5.1022e-01, 1.4615e-01, 3.7762e-01, 3.5555e-01,\n",
       "         2.6241e-01, 4.7632e-01, 3.8910e-01, 4.6211e-01, 7.1702e-01, 2.0004e-01,\n",
       "         7.2053e-01, 5.1364e-01, 6.7404e-01, 7.1665e-01, 5.2356e-01, 8.7721e-01,\n",
       "         3.5720e-01, 3.9345e-01, 9.3270e-01, 5.2297e-01, 8.8642e-01, 7.1024e-01,\n",
       "         7.4385e-01, 1.1979e-01, 2.6973e-01, 1.9735e-02, 1.3584e-01, 4.8917e-01,\n",
       "         3.6572e-01, 7.3799e-01, 7.4818e-02, 5.1560e-01, 3.9652e-01, 5.8992e-01,\n",
       "         3.3390e-01, 9.3320e-01, 8.7195e-01, 5.8544e-01, 9.7087e-01, 3.2869e-01,\n",
       "         8.2148e-01, 6.7177e-01, 8.1406e-01, 1.5467e-01, 1.9476e-01, 3.0521e-01,\n",
       "         6.7711e-01, 3.1570e-01, 1.9117e-01, 6.8540e-02, 4.2068e-01, 6.8766e-01,\n",
       "         9.3085e-01, 6.9797e-01, 9.3345e-01, 6.4553e-01, 2.3004e-01, 6.4400e-01,\n",
       "         7.2139e-02, 3.4566e-01, 4.2620e-01, 5.2582e-01, 9.9181e-01, 2.4633e-01,\n",
       "         5.9191e-01, 4.1960e-01, 1.8972e-02, 6.7101e-01, 9.9335e-01, 2.8646e-01,\n",
       "         6.7698e-01, 7.3844e-01, 6.8885e-04, 2.3529e-01, 7.9757e-01, 7.9174e-01,\n",
       "         4.5433e-01, 5.2393e-01, 4.9304e-01, 6.1693e-01, 2.0616e-01, 7.7846e-01,\n",
       "         7.9057e-02, 5.6802e-02, 6.8593e-01, 9.4221e-01, 7.5692e-02, 4.6590e-02,\n",
       "         4.2317e-02, 3.2258e-01, 3.6581e-01, 1.0870e-01, 5.2231e-01, 5.5689e-01,\n",
       "         4.0773e-01, 8.8787e-01, 2.4875e-01, 1.8633e-01, 4.8475e-01, 3.6798e-01,\n",
       "         4.0154e-01, 6.8796e-01, 1.9345e-01, 8.7327e-01, 1.4211e-01, 7.5562e-01,\n",
       "         7.1402e-01, 2.7816e-01, 3.0331e-01, 5.3498e-01, 7.1803e-01, 8.9606e-01,\n",
       "         7.0481e-01, 9.9185e-01, 9.5385e-01, 6.3091e-01, 2.2906e-01, 9.5779e-01,\n",
       "         8.1991e-01, 9.1516e-02, 7.1808e-01, 2.1329e-01, 8.2127e-01, 3.6074e-01,\n",
       "         1.0233e-01, 1.0666e-01, 6.4154e-01, 3.6350e-01, 9.4463e-01, 5.3696e-01,\n",
       "         3.1301e-01, 7.1124e-01, 2.5665e-01, 6.9008e-01, 2.3198e-01, 6.9037e-01,\n",
       "         1.0213e-01, 6.3936e-01, 4.2102e-01, 8.5518e-01, 9.5317e-01, 6.6331e-01,\n",
       "         5.6183e-01, 3.2833e-01, 7.2211e-01, 6.4507e-01, 2.8920e-01, 5.5733e-01,\n",
       "         6.3352e-01, 6.0652e-01, 7.5682e-01, 9.9377e-01, 7.9424e-01, 2.0418e-01,\n",
       "         4.3897e-02, 2.1230e-02, 3.7702e-01, 9.5629e-01, 4.9026e-01, 9.1541e-01,\n",
       "         8.8693e-01, 7.0988e-01, 4.9958e-01, 9.5642e-01, 8.3075e-01, 5.8294e-01,\n",
       "         9.4325e-01, 2.2434e-01, 8.3504e-01, 4.9509e-01, 7.6373e-01, 4.3019e-01,\n",
       "         2.8031e-01, 9.2140e-01, 6.6645e-01, 6.0805e-01, 3.2134e-01, 6.2498e-01,\n",
       "         9.7622e-02, 4.7369e-01, 9.1675e-01, 8.2162e-01, 6.8263e-01, 4.3019e-01,\n",
       "         3.0964e-01, 8.4314e-01, 2.6046e-01, 7.8249e-01, 9.8512e-01, 4.8997e-01,\n",
       "         5.3959e-01, 6.2371e-01, 7.3341e-01, 5.4850e-01, 4.2669e-01, 6.1879e-01,\n",
       "         8.2465e-01, 6.8490e-01, 7.2193e-01, 4.4964e-01, 7.1773e-01, 5.9118e-01,\n",
       "         9.7423e-01, 1.3381e-01, 2.0888e-01, 5.9126e-01, 1.5838e-01, 7.7603e-01,\n",
       "         7.5342e-01, 2.7244e-01, 5.5451e-01, 2.5196e-01, 8.2424e-01, 8.8825e-01,\n",
       "         3.7474e-01, 6.3964e-01, 2.3097e-01, 4.9860e-01, 2.7829e-01, 8.6879e-01,\n",
       "         7.8593e-01, 3.3145e-01, 5.4682e-01, 6.4217e-01, 8.7285e-01, 6.4058e-01,\n",
       "         4.3727e-01, 5.1667e-01, 8.1524e-02, 4.7594e-01, 4.1337e-01, 8.8111e-01,\n",
       "         6.9485e-01, 8.0793e-01, 3.0966e-01, 7.0481e-01, 3.6827e-01, 5.2678e-01,\n",
       "         7.2931e-01, 7.3256e-01, 7.1435e-01, 2.3916e-01, 5.0261e-01, 7.8970e-01,\n",
       "         3.5147e-01, 2.2039e-01, 9.1314e-01, 6.0375e-01, 8.0136e-01, 7.3631e-01,\n",
       "         1.0477e-01, 4.1548e-01, 6.9176e-02, 1.4466e-01, 5.1715e-01, 4.0944e-01,\n",
       "         6.3647e-01, 3.8032e-02, 3.9909e-01, 3.5575e-01, 3.5797e-01, 1.4898e-01,\n",
       "         5.7522e-01, 9.1403e-01, 4.0462e-01, 7.3111e-01, 5.2185e-01, 2.0935e-02,\n",
       "         2.4566e-01, 6.2951e-01, 8.7888e-01, 1.2917e-01, 9.7579e-01, 7.4410e-01,\n",
       "         7.5932e-01, 3.7166e-01, 2.4346e-01, 2.6245e-01, 2.4523e-01, 6.0012e-01,\n",
       "         8.0483e-01, 1.9536e-01, 1.7803e-01, 1.9413e-01, 2.9473e-01, 9.9038e-01,\n",
       "         1.5860e-01, 5.0908e-01, 8.0019e-01, 1.7645e-01, 8.3871e-01, 4.3719e-01,\n",
       "         7.9044e-01, 4.5775e-01, 3.4045e-01, 8.4818e-01, 7.8821e-01, 9.5677e-01,\n",
       "         7.8646e-01, 1.7539e-01, 5.6793e-01, 8.5897e-01, 8.1024e-02, 5.1908e-01,\n",
       "         1.8462e-01, 4.6032e-01, 5.7004e-01, 7.2243e-01, 3.6218e-01, 3.5954e-01,\n",
       "         9.7142e-01, 3.4407e-01, 5.8174e-04, 3.7910e-01, 2.7302e-01, 4.3885e-01,\n",
       "         9.7709e-01, 7.1787e-01, 6.8497e-01, 9.6477e-01, 4.8128e-01, 3.5713e-01,\n",
       "         1.7608e-01, 5.9251e-01, 2.6205e-01, 8.9805e-01, 8.7550e-01, 9.0427e-01,\n",
       "         3.6160e-01, 2.3267e-01, 1.4183e-01, 3.7657e-01, 6.6138e-01, 2.8831e-01,\n",
       "         4.6058e-01, 5.2105e-01, 4.2955e-02, 4.9268e-02, 2.4855e-01, 7.8681e-01,\n",
       "         1.8304e-01, 1.3438e-01, 2.0765e-01, 9.7259e-01, 4.8619e-01, 3.5243e-01,\n",
       "         9.1202e-01, 5.6599e-01, 6.8730e-01, 5.2593e-01, 9.3239e-01, 4.1360e-01,\n",
       "         8.5651e-01, 5.6622e-01, 7.0421e-02, 6.4349e-01, 6.3882e-02, 6.9556e-01,\n",
       "         3.3976e-01, 6.5660e-01, 6.8629e-01, 8.8019e-01, 6.2266e-01, 4.4651e-01,\n",
       "         6.8222e-01, 1.7900e-01, 4.6921e-01, 5.4035e-01, 7.7063e-01, 1.6183e-01,\n",
       "         2.2932e-01, 8.8678e-01, 2.8597e-01, 3.8686e-01, 7.8353e-01, 7.6090e-01,\n",
       "         9.2407e-01, 5.4961e-01, 7.7283e-01, 8.2233e-02, 1.1387e-03, 3.3507e-01,\n",
       "         8.7322e-01, 2.6649e-02, 1.9353e-01, 6.6297e-01, 9.0342e-01, 1.9050e-01,\n",
       "         4.5479e-01, 2.2297e-01, 2.3487e-02, 4.0513e-01, 9.7980e-01, 6.8544e-01,\n",
       "         2.8520e-01, 4.9513e-01, 4.5205e-01, 4.3572e-01, 6.6075e-01, 2.8496e-01,\n",
       "         6.9062e-01, 3.7265e-01, 8.1759e-01, 7.1837e-01, 1.8437e-01, 5.0255e-01,\n",
       "         6.7477e-01, 4.6608e-01, 5.4020e-01, 5.4392e-01, 1.5063e-01, 7.3486e-02,\n",
       "         1.8784e-01, 6.0471e-01, 8.0442e-01, 4.8022e-02, 3.0579e-01, 4.0363e-01,\n",
       "         1.3965e-01, 1.9101e-01, 4.8716e-01, 1.3374e-01, 3.4179e-01, 6.0663e-01,\n",
       "         5.8775e-01, 9.2580e-01, 7.1595e-01, 2.9007e-02, 9.4597e-01, 3.3530e-01,\n",
       "         6.0175e-02, 8.6220e-01, 3.5044e-01, 8.8081e-01, 8.6419e-01, 9.5514e-01,\n",
       "         9.7903e-01, 2.7093e-01, 6.9586e-02, 6.7064e-02, 8.2874e-01, 3.1970e-01,\n",
       "         5.2820e-01, 2.1512e-01, 6.5615e-01, 9.4187e-02, 7.5659e-02, 5.2819e-01,\n",
       "         2.8433e-01, 4.4602e-01, 9.8933e-01, 1.7655e-01, 4.1495e-01, 8.2920e-01,\n",
       "         4.9258e-01, 6.2412e-01, 7.7153e-01, 2.8109e-01, 4.7418e-01, 5.0160e-02,\n",
       "         3.4886e-01, 7.7934e-01, 1.0788e-01, 2.8048e-01, 9.2353e-01, 8.1898e-01,\n",
       "         1.9451e-01, 6.5971e-01, 7.1948e-02, 3.7660e-01, 3.4810e-01, 8.8409e-01,\n",
       "         5.0972e-02, 3.2207e-02, 7.0636e-01, 1.7002e-01, 2.7411e-01, 4.0061e-01,\n",
       "         1.8601e-01, 4.0489e-01, 8.1682e-02, 8.7148e-01, 3.6256e-01, 9.0667e-01,\n",
       "         1.7293e-01, 2.1403e-01, 8.8861e-01, 2.5084e-01, 5.5779e-01, 5.8121e-01,\n",
       "         9.7796e-01, 1.4293e-01, 4.1914e-02, 3.6451e-01, 9.5647e-01, 7.9254e-01,\n",
       "         6.5842e-01, 4.7582e-03, 8.3358e-01, 1.9002e-01, 1.9417e-01, 4.3289e-01,\n",
       "         6.7102e-01, 1.5630e-02, 9.8749e-01, 3.0879e-01, 2.7566e-01, 6.0375e-01,\n",
       "         8.5911e-01, 3.6664e-01, 5.0318e-01, 3.4316e-01, 7.5445e-01, 4.7261e-01,\n",
       "         4.8295e-01, 3.2294e-01, 1.2085e-01, 8.0287e-01, 8.6539e-02, 6.9262e-01,\n",
       "         6.1392e-01, 2.4249e-01, 9.1627e-01, 3.8386e-02, 7.1992e-01, 2.6453e-01,\n",
       "         3.7498e-02, 6.3928e-01, 5.1943e-01, 6.6143e-01, 4.0855e-02, 1.8830e-01,\n",
       "         1.9562e-01, 3.8759e-01, 1.2814e-01, 8.7003e-01, 8.0783e-02, 2.1376e-01,\n",
       "         8.7602e-01, 8.9278e-01, 9.3710e-01, 5.1186e-01, 6.5485e-01, 2.5575e-01,\n",
       "         7.1224e-01, 6.6002e-01, 5.0602e-01, 5.0155e-01, 5.4111e-01, 4.9365e-01,\n",
       "         3.3483e-01, 9.8339e-01, 5.2791e-01, 3.3638e-01, 7.6844e-01, 8.7084e-01,\n",
       "         5.3713e-01, 9.9716e-03, 3.9938e-01, 6.9824e-01, 5.2743e-01, 3.5198e-01,\n",
       "         9.4627e-01, 4.2366e-01, 8.8091e-01, 4.7915e-02, 3.1281e-01, 7.4739e-01,\n",
       "         4.8228e-01, 8.5535e-01, 3.4680e-01, 6.2372e-02, 2.7063e-01, 2.9588e-01,\n",
       "         7.7944e-01, 6.2960e-01, 8.2740e-01, 8.6152e-01, 6.2315e-01, 5.2867e-01,\n",
       "         8.4782e-01, 6.5698e-01, 3.5962e-01, 2.7945e-01, 2.5563e-01, 7.3733e-01,\n",
       "         1.8274e-01, 8.1665e-01, 7.1763e-01, 7.5342e-02, 6.2818e-01, 6.5480e-01,\n",
       "         7.5943e-01, 2.6528e-01, 2.1909e-01, 7.7531e-01, 5.9118e-01, 1.8545e-01,\n",
       "         1.7955e-01, 4.0418e-01, 4.6907e-01, 6.5472e-01, 3.9778e-01, 2.0265e-01,\n",
       "         2.6426e-01, 9.3539e-01, 9.3082e-01, 1.5405e-01, 4.6158e-01, 4.8617e-01,\n",
       "         5.0459e-01, 1.9819e-01, 1.5052e-01, 3.1589e-01, 4.0763e-01, 2.1046e-01,\n",
       "         7.4098e-01, 5.6810e-01, 5.1048e-01, 5.3573e-01, 6.3379e-01, 6.9405e-01,\n",
       "         7.0644e-01, 9.5341e-01, 5.1464e-01, 4.0576e-01, 1.1451e-01, 4.8089e-01,\n",
       "         2.9216e-01, 9.3687e-01, 2.7251e-01, 8.5532e-02, 5.4393e-01, 1.5031e-01,\n",
       "         3.6841e-01, 7.0467e-01, 8.7738e-01, 3.6983e-01, 4.4422e-02, 3.1840e-01,\n",
       "         9.0192e-01, 5.3355e-01, 8.7346e-01, 8.0146e-01, 1.7867e-01, 2.7395e-02,\n",
       "         4.8303e-01, 8.4830e-01, 4.6899e-01, 3.4972e-01, 4.9210e-01, 5.4047e-01,\n",
       "         7.7227e-01, 2.8383e-01, 2.1447e-01, 5.4180e-01, 2.3800e-01, 9.5798e-01,\n",
       "         4.1883e-02, 9.1443e-01, 9.9486e-01, 2.2448e-01, 5.1427e-01, 7.2335e-01,\n",
       "         6.1940e-01, 8.1539e-01, 4.7239e-01, 6.8194e-01, 9.7828e-01, 9.3219e-02,\n",
       "         8.2110e-01, 8.6769e-01, 2.3801e-01, 1.2699e-01, 8.4623e-01, 8.2910e-01,\n",
       "         8.8340e-01, 1.8871e-01, 5.7647e-02, 6.4826e-01, 4.2671e-01, 2.5319e-01,\n",
       "         8.5469e-01, 7.2414e-01, 6.7468e-01, 5.3247e-01, 9.9610e-01, 2.2909e-01,\n",
       "         6.7287e-01, 4.9015e-02, 6.0467e-01, 1.1160e-01, 8.3856e-01, 4.1680e-01,\n",
       "         3.6677e-01, 5.0409e-01, 4.4359e-01, 1.6924e-01, 4.6499e-01, 2.7009e-01,\n",
       "         3.3305e-01, 6.2364e-01, 2.1749e-01, 7.0813e-02, 3.3061e-02, 9.5131e-03,\n",
       "         4.8264e-01, 4.2857e-01, 2.5338e-01, 9.4662e-01, 4.9612e-01, 5.4944e-01,\n",
       "         9.6500e-01, 9.0152e-02, 4.9768e-01, 7.0614e-01, 4.9900e-01, 6.5400e-01,\n",
       "         5.2342e-01, 2.1876e-01, 6.1527e-01, 9.2822e-01, 1.7250e-01, 3.2356e-01,\n",
       "         7.7525e-01, 2.4033e-02, 6.7898e-01, 3.7469e-01, 4.1242e-01, 3.7303e-02,\n",
       "         2.8994e-01, 8.7849e-01, 6.7365e-01, 1.3924e-01, 8.3773e-01, 4.1156e-01,\n",
       "         4.3798e-01, 8.0992e-02, 1.0884e-01, 3.9367e-01, 8.7833e-02, 3.3077e-01,\n",
       "         3.5087e-01, 5.7718e-01, 8.0891e-01, 8.9084e-02, 2.1677e-01, 7.2621e-02,\n",
       "         3.5132e-01, 3.1573e-01, 6.0731e-01, 7.9012e-01, 1.0637e-01, 6.9455e-01,\n",
       "         5.1648e-01, 8.8225e-01, 2.4879e-01, 7.5416e-01, 6.5996e-02, 5.2668e-01,\n",
       "         9.0385e-01, 9.8437e-01, 4.9495e-01, 3.0510e-01, 4.7084e-01, 6.6234e-01,\n",
       "         7.9906e-01, 6.5633e-01, 3.5089e-01, 7.3513e-01, 7.6928e-01, 8.7723e-01,\n",
       "         8.9744e-02, 1.2052e-01, 8.0859e-01, 4.4155e-01, 6.6981e-01, 1.0374e-01,\n",
       "         7.9232e-01, 3.2654e-01, 5.8456e-01, 2.6880e-01, 9.0331e-02, 2.6076e-01,\n",
       "         9.2988e-01, 2.2154e-01, 7.3421e-01, 2.7164e-01, 4.0584e-01, 6.0265e-01,\n",
       "         7.7208e-01, 5.9738e-02, 5.5888e-01, 6.6245e-01, 9.5720e-01, 4.7406e-02,\n",
       "         2.8480e-01, 8.7065e-01, 8.3796e-01, 3.2571e-01, 5.9911e-01, 1.0986e-01,\n",
       "         7.0680e-01, 7.5867e-01, 4.6148e-01, 7.9694e-01, 4.9061e-01, 8.6097e-01,\n",
       "         2.6650e-01, 5.8052e-01, 3.5018e-01, 6.3677e-01, 6.2412e-01, 9.9794e-01,\n",
       "         3.9677e-01, 1.9675e-01, 5.4738e-01, 7.8750e-01, 8.4994e-01, 7.5836e-01,\n",
       "         5.1023e-01, 2.9026e-01, 4.2148e-01, 9.8912e-01, 4.4540e-01, 7.3375e-01,\n",
       "         3.3391e-01, 2.0198e-01, 1.9245e-01, 7.5059e-01, 5.9930e-01, 8.4105e-01,\n",
       "         6.0836e-02, 3.5926e-02, 9.7264e-01, 3.5693e-01, 9.3713e-02, 5.8739e-01,\n",
       "         6.7932e-01, 7.2566e-01, 8.7542e-01, 9.0280e-01, 8.2306e-01, 2.6673e-02,\n",
       "         5.2038e-01, 7.9724e-01, 1.6498e-01, 7.6247e-01, 8.8180e-01, 3.4272e-01,\n",
       "         5.7619e-01, 8.6870e-01, 2.0514e-01, 1.2496e-01, 2.7824e-01, 5.3931e-01,\n",
       "         9.9512e-01, 3.8687e-01, 7.1978e-01, 2.0766e-02, 4.9843e-01, 2.3522e-01,\n",
       "         3.6933e-01, 2.2667e-01, 7.7639e-01, 1.2035e-01, 4.5837e-01, 7.8754e-02,\n",
       "         9.1593e-01, 1.9319e-01, 1.1532e-02, 4.3748e-01, 5.8045e-01, 6.1251e-01,\n",
       "         5.5106e-01, 6.1896e-01, 2.0277e-01, 3.7532e-01, 8.1801e-02, 2.9438e-01,\n",
       "         4.1154e-01, 8.9049e-01, 7.5411e-01, 6.9420e-01, 7.6768e-01, 5.8552e-01,\n",
       "         5.6780e-01, 9.6445e-01, 4.2888e-01, 7.6006e-02, 5.3222e-01, 9.7164e-01,\n",
       "         7.6529e-01, 9.8600e-01, 3.1169e-01, 1.9425e-01, 4.0454e-01, 3.5175e-01,\n",
       "         8.8120e-01, 4.5389e-01, 6.3468e-01, 9.6736e-01, 8.2493e-01, 4.1978e-01,\n",
       "         3.9577e-01, 6.8064e-01, 9.7065e-01, 3.1018e-01, 9.1482e-01, 1.5654e-01,\n",
       "         8.0098e-01, 2.8576e-01, 1.2350e-01, 8.4340e-02, 3.6219e-01, 6.1180e-01,\n",
       "         1.5025e-01, 2.9949e-02, 2.2801e-01, 9.2715e-01, 1.0223e-01, 4.8256e-01,\n",
       "         5.9539e-01, 5.9974e-01, 6.1144e-01, 7.5240e-01, 6.5510e-01, 1.9246e-01,\n",
       "         7.9662e-01, 7.8984e-01, 6.9938e-01, 9.6590e-01, 8.4408e-01, 2.8415e-01,\n",
       "         7.1506e-01, 7.8426e-01, 5.1917e-01, 9.2024e-01, 7.2121e-01, 1.2173e-01,\n",
       "         6.0124e-01, 3.1949e-01, 4.7224e-01, 2.9590e-01, 7.2657e-01, 5.9086e-01,\n",
       "         2.1810e-01, 2.2657e-01, 4.9094e-01, 3.6911e-02, 2.3686e-01, 5.2526e-01,\n",
       "         3.9443e-01, 6.3015e-01, 8.1900e-01, 2.6097e-01, 9.0868e-02, 4.3308e-01,\n",
       "         9.5110e-01, 3.7013e-03, 4.0235e-01, 6.8481e-01]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-498.7801, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
